{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start by loading some libraries.\n",
    "# The convention is to load everything here at the start of the notebook\n",
    "# so it is easier to read the notebook.\n",
    "# For this notebook, however, we will load the packages as we need them, \n",
    "# so you can learn when we need them.\n",
    "\n",
    "import pandas as pd\n",
    "import altair as alt \n",
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.neighbors \n",
    "#alt.data_transformers.enable(\"vegafusion\")\n",
    "alt.data_transformers.disable_max_rows()\n",
    "\n",
    "penguins_raw = pd.read_csv(\"https://raw.githubusercontent.com/mcnakhaee/palmerpenguins/master/palmerpenguins/data/penguins.csv\")\n",
    "\n",
    "\n",
    "\n",
    "## DO NOT WORRY ABOUT THIS FUNCTION HERE\n",
    "## I CREATED TO PLOT THE CLASSIFICATION GRID \n",
    "## JUST IGNORE IT.\n",
    "def plot_grid_rl(my_knn: sklearn.neighbors.KNeighborsClassifier, data_df: pd.DataFrame, covariates: list[str], response: str):\n",
    "  \"\"\"\n",
    "  Create a visualization of KNN classification results on a grid.\n",
    "\n",
    "  This function generates a scatter plot of data points and overlays\n",
    "  it with a grid of predicted classifications using a KNN model.\n",
    "\n",
    "  Parameters:\n",
    "  -----------\n",
    "  my_knn : KNN\n",
    "      A trained K-Nearest Neighbors classifier object.\n",
    "  data_df : pd.DataFrame\n",
    "      The dataset containing the original data points.\n",
    "  covariates : list[str]\n",
    "      A list of two column names to be used as covariates (features) for the plot.\n",
    "  response : str\n",
    "      The name of the column containing the response variable (target).\n",
    "\n",
    "  Returns:\n",
    "  --------\n",
    "  alt.LayerChart\n",
    "      An Altair chart object containing the scatter plot of original data\n",
    "      and the grid of KNN predictions.\n",
    "\n",
    "  Dependencies:\n",
    "  -------------\n",
    "  - numpy (imported as np)\n",
    "  - pandas (imported as pd)\n",
    "  - altair (imported as alt)\n",
    "  \"\"\"\n",
    "\n",
    "  # Calculate min and max values for the covariates\n",
    "  min_max_info = data_df[covariates].agg(['min', 'max'])\n",
    "\n",
    "  # Create a grid of points for prediction, extending 5% beyond the data range\n",
    "  grid = np.meshgrid(np.linspace(min_max_info.iloc[0,0]-0.05*np.abs(min_max_info.iloc[0,0]), 1.05*min_max_info.iloc[1,0], 200), \n",
    "                     np.linspace(min_max_info.iloc[0,1]-0.05*np.abs(min_max_info.iloc[0,1]), 1.05*min_max_info.iloc[1,1], 200))\n",
    "\n",
    "  # Convert the grid to a DataFrame for easier processing\n",
    "  grid_df = pd.DataFrame({\n",
    "      covariates[0]: grid[0].ravel(),\n",
    "      covariates[1]: grid[1].ravel()\n",
    "  })\n",
    "\n",
    "  # Use the KNN model to predict the response for each point in the grid\n",
    "  grid_df['predicted_'+response] = my_knn.predict(grid_df)\n",
    "\n",
    "  # Create the base scatter plot of original data\n",
    "  base_chart = alt.Chart(data_df).mark_point().encode(\n",
    "      x=alt.X(covariates[0]).scale(zero=False, domain=(min_max_info.iloc[0,0]-0.05*np.abs(min_max_info.iloc[0,0]), 1.05*min_max_info.iloc[1,0])),\n",
    "      y=alt.Y(covariates[1]).scale(zero=False, domain=(min_max_info.iloc[0,1]-0.05*np.abs(min_max_info.iloc[0,1]), 1.05*min_max_info.iloc[1,1])),\n",
    "      color=response+':N'\n",
    "  )\n",
    "\n",
    "  # Create the overlay of predicted classifications\n",
    "  prediction_layer = alt.Chart(grid_df).mark_point(\n",
    "      opacity=0.05,\n",
    "      size=40,\n",
    "      filled=True\n",
    "  ).encode(\n",
    "      x=alt.X(covariates[0]).scale(zero=False, domain=(min_max_info.iloc[0,0]-0.05*np.abs(min_max_info.iloc[0,0]), 1.05*min_max_info.iloc[1,0])),\n",
    "      y=alt.Y(covariates[1]).scale(zero=False, domain=(min_max_info.iloc[0,1]-0.05*np.abs(min_max_info.iloc[0,1]), 1.05*min_max_info.iloc[1,1])),\n",
    "      color=alt.Color('predicted_'+response+':N').title(response)\n",
    "  )\n",
    "\n",
    "  # Combine the base chart and prediction layer\n",
    "  return (base_chart + prediction_layer).properties(width=800, height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [1] \n",
    "# We will work with the penguins' data set, let's take a look:\n",
    "\n",
    "penguins_raw.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [2]\n",
    "# Does this data contain NA?\n",
    "\n",
    "penguins_raw[penguins_raw.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [3] \n",
    "# How many? \n",
    "\n",
    "penguins_raw.isna().any(axis=1).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [4]\n",
    "# Since just a tiny proportion of rows have NAs, let's just drop these rows.\n",
    "\n",
    "penguins_clean = penguins_raw.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [5]\n",
    "# Let's take a look at our data\n",
    "\n",
    "alt.Chart(penguins_clean)\\\n",
    "   .mark_point()\\\n",
    "   .encode(\n",
    "       x=alt.X('body_mass_g').title(\"Body Mass (g)\").scale(zero=False),\n",
    "       y=alt.Y(\"bill_depth_mm\").title(\"Bill Depth\").scale(zero=False),\n",
    "       color='species'\n",
    "   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [6]\n",
    "# It seems that Body Mass and Bill Depth are able to identify the Gentoo penguin pretty well\n",
    "# but they are not useful to distinguish between Chinstrap and Adelie penguins. \n",
    "# Nonetheless, let's try to fit our KNN Classifier. \n",
    "\n",
    "# The first step is to create our classify and define the so-called hyperparameters. \n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN # this loads the package\n",
    "\n",
    "my_knn = KNN(n_neighbors=5) # this line creates the classified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [7]\n",
    "# Now we are ready to \"train\" the algorithm. \n",
    "\n",
    "my_knn.fit(penguins_clean[['body_mass_g', 'bill_depth_mm']], penguins_clean['species'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [8]\n",
    "# Now, our classifier is trained. Let's use it for some predictions.\n",
    "\n",
    "my_knn.predict(penguins_clean[['body_mass_g', 'bill_depth_mm']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [9]\n",
    "# We can just call the predict function\n",
    "\n",
    "my_knn.predict(penguins_clean[['body_mass_g', 'bill_depth_mm']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [10]\n",
    "# Let's take a look at the prediction regions:\n",
    "\n",
    "plot_grid_rl(my_knn, penguins_clean, covariates=['body_mass_g', 'bill_depth_mm'], response='species' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [11]\n",
    "\n",
    "# Look closely at the plot above. \n",
    "# It doesn't make much sense, does it?. \n",
    "# The problem is the scale of the variables. \n",
    "# The horizontal distance dominates, so the vertical distance is not really relevant. \n",
    "\n",
    "# We need to scale the variables. \n",
    "# We could do this using pandas operations\n",
    "\n",
    "mean_body_mass = penguins_clean['body_mass_g'].mean()\n",
    "std_body_mass = penguins_clean['body_mass_g'].std(ddof=0)\n",
    "mean_bill_depth = penguins_clean['bill_depth_mm'].mean()\n",
    "std_bill_depth = penguins_clean['bill_depth_mm'].std(ddof=0)\n",
    "\n",
    "penguins_clean_std = penguins_clean.assign(\n",
    "    body_mass_g = (penguins_clean['body_mass_g']-mean_body_mass)/std_body_mass,\n",
    "    bill_depth_mm = (penguins_clean['bill_depth_mm'] - mean_bill_depth)/std_bill_depth \n",
    ")\n",
    "\n",
    "penguins_clean_std[['body_mass_g', 'bill_depth_mm']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [12]\n",
    "\n",
    "# We should not do this manually. \n",
    "# scikit-learn has some tools to help us do this more conveniently, less error-prone and avoid data leakage (more on next week).\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit_transform(penguins_clean[['body_mass_g', 'bill_depth_mm']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [13]\n",
    "# One inconvenient thing here, though, is that the output is a numpy array, not a pandas data frame. \n",
    "# (Numpy arrays are different data structures.)\n",
    "\n",
    "# We can change this behaviour in scikit-learn with set_config\n",
    "from sklearn import set_config\n",
    "set_config(transform_output='pandas')\n",
    "\n",
    "scaler.fit_transform(penguins_clean[['body_mass_g', 'bill_depth_mm']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [14]\n",
    "#\n",
    "# There you go, much better! \n",
    "# Let's fit again our knn\n",
    "\n",
    "my_knn.fit(penguins_clean_std[['body_mass_g', 'bill_depth_mm']], penguins_clean_std['species'])\n",
    "\n",
    "plot_grid_rl(my_knn, penguins_clean_std, covariates=['body_mass_g', 'bill_depth_mm'], response='species' ).properties(title=\"Isn't this better?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [15]\n",
    "\n",
    "# Note that here, we had to first scale our data, then we passed our scaled data into our model to fit.\n",
    "# Then, to predict, we need to scale the new data and then pass the scaled new data to the model for prediction. \n",
    "# scikit-learn allows us to create a pipeline to make this more convenient. \n",
    "\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "\n",
    "my_pipeline = Pipeline([\n",
    "    ('scaler', scaler),\n",
    "    ('knn_model', KNN(n_neighbors=15))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [16]\n",
    "\n",
    "# Now we can do everything with one line only:\n",
    "my_pipeline.fit(penguins_clean[['body_mass_g', 'bill_depth_mm']], penguins_clean['species'] )\n",
    "\n",
    "# We can even pass the pipeline instead of the model to functions, look: \n",
    "plot_grid_rl(my_pipeline, penguins_clean, covariates=['body_mass_g', 'bill_depth_mm'], response='species' ).properties(title=\"Isn't this better?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start by loading some libraries.\n",
    "# The convention is to load everything here at the start of the notebook\n",
    "# so it is easier to read the notebook.\n",
    "# For this notebook, however, we will load the packages as we need them, \n",
    "# so you can learn when we need them.\n",
    "\n",
    "import pandas as pd\n",
    "import altair as alt \n",
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.neighbors \n",
    "alt.data_transformers.enable(\"vegafusion\")\n",
    "\n",
    "penguins_raw = pd.read_csv(\"https://raw.githubusercontent.com/mcnakhaee/palmerpenguins/master/palmerpenguins/data/penguins.csv\")\n",
    "\n",
    "\n",
    "\n",
    "## DO NOT WORRY ABOUT THIS FUNCTION HERE\n",
    "## I CREATED TO PLOT THE CLASSIFICATION GRID \n",
    "## JUST IGNORE IT.\n",
    "def plot_grid_rl(my_knn: sklearn.neighbors.KNeighborsClassifier, data_df: pd.DataFrame, covariates: list[str], response: str):\n",
    "  \"\"\"\n",
    "  Create a visualization of KNN classification results on a grid.\n",
    "\n",
    "  This function generates a scatter plot of data points and overlays\n",
    "  it with a grid of predicted classifications using a KNN model.\n",
    "\n",
    "  Parameters:\n",
    "  -----------\n",
    "  my_knn : KNN\n",
    "      A trained K-Nearest Neighbors classifier object.\n",
    "  data_df : pd.DataFrame\n",
    "      The dataset containing the original data points.\n",
    "  covariates : list[str]\n",
    "      A list of two column names to be used as covariates (features) for the plot.\n",
    "  response : str\n",
    "      The name of the column containing the response variable (target).\n",
    "\n",
    "  Returns:\n",
    "  --------\n",
    "  alt.LayerChart\n",
    "      An Altair chart object containing the scatter plot of original data\n",
    "      and the grid of KNN predictions.\n",
    "\n",
    "  Dependencies:\n",
    "  -------------\n",
    "  - numpy (imported as np)\n",
    "  - pandas (imported as pd)\n",
    "  - altair (imported as alt)\n",
    "  \"\"\"\n",
    "\n",
    "  # Calculate min and max values for the covariates\n",
    "  min_max_info = data_df[covariates].agg(['min', 'max'])\n",
    "\n",
    "  # Create a grid of points for prediction, extending 5% beyond the data range\n",
    "  grid = np.meshgrid(np.linspace(min_max_info.iloc[0,0]-0.05*np.abs(min_max_info.iloc[0,0]), 1.05*min_max_info.iloc[1,0], 200), \n",
    "                     np.linspace(min_max_info.iloc[0,1]-0.05*np.abs(min_max_info.iloc[0,1]), 1.05*min_max_info.iloc[1,1], 200))\n",
    "\n",
    "  # Convert the grid to a DataFrame for easier processing\n",
    "  grid_df = pd.DataFrame({\n",
    "      covariates[0]: grid[0].ravel(),\n",
    "      covariates[1]: grid[1].ravel()\n",
    "  })\n",
    "\n",
    "  # Use the KNN model to predict the response for each point in the grid\n",
    "  grid_df['predicted_'+response] = my_knn.predict(grid_df)\n",
    "\n",
    "  # Create the base scatter plot of original data\n",
    "  base_chart = alt.Chart(data_df).mark_point().encode(\n",
    "      x=alt.X(covariates[0]).scale(zero=False, domain=(min_max_info.iloc[0,0]-0.05*np.abs(min_max_info.iloc[0,0]), 1.05*min_max_info.iloc[1,0])),\n",
    "      y=alt.Y(covariates[1]).scale(zero=False, domain=(min_max_info.iloc[0,1]-0.05*np.abs(min_max_info.iloc[0,1]), 1.05*min_max_info.iloc[1,1])),\n",
    "      color=response+':N'\n",
    "  )\n",
    "\n",
    "  # Create the overlay of predicted classifications\n",
    "  prediction_layer = alt.Chart(grid_df).mark_point(\n",
    "      opacity=0.05,\n",
    "      size=40,\n",
    "      filled=True\n",
    "  ).encode(\n",
    "      x=alt.X(covariates[0]).scale(zero=False, domain=(min_max_info.iloc[0,0]-0.05*np.abs(min_max_info.iloc[0,0]), 1.05*min_max_info.iloc[1,0])),\n",
    "      y=alt.Y(covariates[1]).scale(zero=False, domain=(min_max_info.iloc[0,1]-0.05*np.abs(min_max_info.iloc[0,1]), 1.05*min_max_info.iloc[1,1])),\n",
    "      color=alt.Color('predicted_'+response+':N').title(response)\n",
    "  )\n",
    "\n",
    "  # Combine the base chart and prediction layer\n",
    "  return (base_chart + prediction_layer).properties(width=800, height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [1] \n",
    "# Similarly to last time, let's drop the small fractions of NAs we have in the penguins dataset. \n",
    "\n",
    "penguins_clean = penguins_raw.dropna()\n",
    "penguins_clean.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [2]\n",
    "# The first thing we need to do now, is to split between train and test data, and \"hide\" the test data.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "penguins_train, penguins_test = train_test_split(penguins_clean, test_size = 0.3, stratify = penguins_clean['species'])\n",
    "\n",
    "penguins_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [2]\n",
    "# Let's take a look at our data\n",
    "\n",
    "alt.Chart(penguins_train)\\\n",
    "   .mark_point()\\\n",
    "   .encode(\n",
    "       x=alt.X('body_mass_g').title(\"Body Mass (g)\").scale(zero=False),\n",
    "       y=alt.Y(\"bill_depth_mm\").title(\"Bill Depth\").scale(zero=False),\n",
    "       color='species'\n",
    "   )\\\n",
    "   .properties(\n",
    "       width=600,\n",
    "       height=400\n",
    "   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [3]\n",
    "# Once again, we need to standardize our data, otherwise KNN will not work properly. \n",
    "# Let's just create the Standization model (but we won't fit it for now). \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "col_transf = make_column_transformer(\n",
    "    (StandardScaler(), ['bill_depth_mm', 'body_mass_g'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [4]\n",
    "# Let's create our KNN classifier\n",
    "# We start by loading the library:\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN \n",
    "\n",
    "my_knn = KNN() # Unfortunately, I have no idea how many neighbours to use here! \n",
    "\n",
    "# What is the best value of neighbors (hyper-parameter)? \n",
    "# (Note: by not specifying the number of neighbours, sklearn uses 5 as default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [5] \n",
    "# Next, let's create our pipeline. \n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "knn_pipeline_5_neighbors = make_pipeline(col_transf, my_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [6]\n",
    "# Now, we need to assess our model so we can choose k. \n",
    "# However, we cannot assess the classifier on observations we used to fit the model. This is \"cheating\".\n",
    "# We don't want the model to be able to \"remember\" stuff, we want it to be able to \"predict\" stuff.\n",
    "# So, we need cross-validation!\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "# Now, we can use cross-validation to compare models. For example: \n",
    "\n",
    "cross_validate(knn_pipeline_5_neighbors, \n",
    "               penguins_train[['body_mass_g', 'bill_depth_mm']], \n",
    "               penguins_train['species'], \n",
    "               cv=10, \n",
    "               scoring=['accuracy', 'precision_macro', 'recall_macro'])\n",
    "\n",
    "\n",
    "\n",
    "## (Note: don't worry about the `_macro` in front of precision and recall -- I had to put those in there \n",
    "## because this is not a binary classification (two-classes only -- we have three species in this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [7]\n",
    "# It might be helpful to store the cross_validate's results as a pandas.DataFrame:\n",
    "penguins_cv_info_5_neighbors = pd.DataFrame(\n",
    "    cross_validate(knn_pipeline_5_neighbors, \n",
    "               penguins_train[['body_mass_g', 'bill_depth_mm']], \n",
    "               penguins_train['species'], \n",
    "               cv=10, \n",
    "               scoring=['accuracy', 'precision_macro', 'recall_macro'])\n",
    ")\n",
    "penguins_cv_info_5_neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [8]\n",
    "# As you can see we have one measure for each fold. \n",
    "# We need to aggregate this information to assess the model: \n",
    "\n",
    "penguins_cv_info_5_neighbors.describe().loc['mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [9]\n",
    "\n",
    "# But, using cross-validation is purposeless to assess only one model. We use Cross-Validation to compare models on \n",
    "# \"out-of-sample\" performance, so we can select the best model. This is a strategy for choosing the parameters. \n",
    "\n",
    "# For example, we could replicate this code and try to see how a KNN with23 neighbours perform and compare\n",
    "# with the KNN above with 5 neighbours. Let's focus on Accuracy. \n",
    "\n",
    "knn_pipeline_2_neighbors = make_pipeline(col_transf, KNN(2))\n",
    "\n",
    "penguins_cv_info_2_neighbors = pd.DataFrame(\n",
    "    cross_validate(knn_pipeline_2_neighbors, \n",
    "               penguins_train[['body_mass_g', 'bill_depth_mm']], \n",
    "               penguins_train['species'], \n",
    "               cv=10, \n",
    "               scoring=['accuracy'])\n",
    ")\n",
    "penguins_cv_info_2_neighbors.describe().loc['mean']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy went from 0.729 to 0.75. An improvement! \n",
    "\n",
    "What if we use other values for neighbors? This can become tiresome! Luckily, `sklearn` has some helper functions to assist us in this search. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [10]\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "cv_results = pd.DataFrame(\n",
    "    GridSearchCV(estimator=knn_pipeline_5_neighbors, \n",
    "             param_grid={'kneighborsclassifier__n_neighbors': range(1, 50, 1)},\n",
    "             scoring='accuracy',\n",
    "             cv=10)\\\n",
    "    .fit(penguins_train[['body_mass_g','bill_depth_mm']], penguins_train['species'])\\\n",
    "    .cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we can visualize the accuracy as a function of the number of neighbours. \n",
    "alt.Chart(cv_results).mark_line().encode(\n",
    "    x=alt.X('param_kneighborsclassifier__n_neighbors').title('Number of Neighbors'),\n",
    "    y=alt.Y('mean_test_score').title('Accuracy').scale(zero=False)\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
